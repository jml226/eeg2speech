{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EEG-to-Speech 파이프라인 워크플로우\n",
    "\n",
    "이 노트북은 EEG 데이터에서 음성을 생성하는 전체 파이프라인의 워크플로우를 보여줍니다. 데이터 다운로드부터 모델 학습, 평가, 결과 시각화까지의 전체 과정을 단계별로 설명합니다.\n",
    "\n",
    "## 목차\n",
    "1. 환경 설정 및 라이브러리 설치\n",
    "2. 데이터 다운로드 및 준비\n",
    "3. 데이터 탐색 및 시각화\n",
    "4. 모델 구성 요소 초기화\n",
    "5. 모델 학습\n",
    "6. 모델 평가\n",
    "7. 결과 시각화 및 분석\n",
    "8. 결론 및 향후 연구 방향"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "1\n",
      "NVIDIA GeForce RTX 4060\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.cuda.is_available())  # True가 출력되어야 정상 작동\n",
    "print(torch.cuda.device_count())  # 사용 가능한 GPU 개수\n",
    "print(torch.cuda.get_device_name(0))  # GPU 이름 출력\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 환경 설정 및 라이브러리 설치\n",
    "\n",
    "필요한 라이브러리를 설치하고 기본 환경을 설정합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: torch in c:\\users\\user\\appdata\\roaming\\python\\python311\\site-packages (2.5.1+cu121)\n",
      "Requirement already satisfied: torchaudio in c:\\users\\user\\appdata\\roaming\\python\\python311\\site-packages (2.5.1+cu121)\n",
      "Requirement already satisfied: numpy in c:\\programdata\\anaconda3\\lib\\site-packages (1.24.3)\n",
      "Requirement already satisfied: matplotlib in c:\\programdata\\anaconda3\\lib\\site-packages (3.7.2)\n",
      "Requirement already satisfied: scipy in c:\\programdata\\anaconda3\\lib\\site-packages (1.11.1)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\user\\appdata\\roaming\\python\\python311\\site-packages (1.5.0)\n",
      "Requirement already satisfied: mne in c:\\users\\user\\appdata\\roaming\\python\\python311\\site-packages (1.6.0)\n",
      "Requirement already satisfied: librosa in c:\\users\\user\\appdata\\roaming\\python\\python311\\site-packages (0.11.0)\n",
      "Requirement already satisfied: soundfile in c:\\users\\user\\appdata\\roaming\\python\\python311\\site-packages (0.13.1)\n",
      "Requirement already satisfied: tqdm in c:\\programdata\\anaconda3\\lib\\site-packages (4.65.0)\n",
      "Requirement already satisfied: transformers in c:\\programdata\\anaconda3\\lib\\site-packages (4.32.1)\n",
      "Requirement already satisfied: diffusers in c:\\users\\user\\appdata\\roaming\\python\\python311\\site-packages (0.32.2)\n",
      "Requirement already satisfied: accelerate in c:\\users\\user\\appdata\\roaming\\python\\python311\\site-packages (1.5.2)\n",
      "Requirement already satisfied: filelock in c:\\programdata\\anaconda3\\lib\\site-packages (from torch) (3.9.0)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\user\\appdata\\roaming\\python\\python311\\site-packages (from torch) (4.13.0)\n",
      "Requirement already satisfied: networkx in c:\\programdata\\anaconda3\\lib\\site-packages (from torch) (3.1)\n",
      "Requirement already satisfied: jinja2 in c:\\programdata\\anaconda3\\lib\\site-packages (from torch) (3.1.2)\n",
      "Requirement already satisfied: fsspec in c:\\users\\user\\appdata\\roaming\\python\\python311\\site-packages (from torch) (2025.3.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\user\\appdata\\roaming\\python\\python311\\site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from matplotlib) (1.0.5)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\programdata\\anaconda3\\lib\\site-packages (from matplotlib) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from matplotlib) (4.25.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from matplotlib) (1.4.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from matplotlib) (23.1)\n",
      "Requirement already satisfied: pillow>=6.2.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from matplotlib) (9.4.0)\n",
      "Requirement already satisfied: pyparsing<3.1,>=2.3.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from matplotlib) (3.0.9)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\programdata\\anaconda3\\lib\\site-packages (from matplotlib) (2.8.2)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from scikit-learn) (1.2.0)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\user\\appdata\\roaming\\python\\python311\\site-packages (from scikit-learn) (3.5.0)\n",
      "Requirement already satisfied: pooch>=1.5 in c:\\users\\user\\appdata\\roaming\\python\\python311\\site-packages (from mne) (1.8.0)\n",
      "Requirement already satisfied: decorator in c:\\programdata\\anaconda3\\lib\\site-packages (from mne) (5.1.1)\n",
      "Requirement already satisfied: lazy-loader>=0.3 in c:\\users\\user\\appdata\\roaming\\python\\python311\\site-packages (from mne) (0.3)\n",
      "Requirement already satisfied: defusedxml in c:\\programdata\\anaconda3\\lib\\site-packages (from mne) (0.7.1)\n",
      "Requirement already satisfied: audioread>=2.1.9 in c:\\users\\user\\appdata\\roaming\\python\\python311\\site-packages (from librosa) (3.0.1)\n",
      "Requirement already satisfied: numba>=0.51.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from librosa) (0.57.1)\n",
      "Requirement already satisfied: soxr>=0.3.2 in c:\\users\\user\\appdata\\roaming\\python\\python311\\site-packages (from librosa) (0.5.0.post1)\n",
      "Requirement already satisfied: msgpack>=1.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from librosa) (1.0.3)\n",
      "Requirement already satisfied: cffi>=1.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from soundfile) (1.15.1)\n",
      "Requirement already satisfied: colorama in c:\\programdata\\anaconda3\\lib\\site-packages (from tqdm) (0.4.6)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.15.1 in c:\\users\\user\\appdata\\roaming\\python\\python311\\site-packages (from transformers) (0.29.3)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers) (2022.7.9)\n",
      "Requirement already satisfied: requests in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers) (2.31.0)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers) (0.13.2)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in c:\\users\\user\\appdata\\roaming\\python\\python311\\site-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: importlib-metadata in c:\\programdata\\anaconda3\\lib\\site-packages (from diffusers) (6.0.0)\n",
      "Requirement already satisfied: psutil in c:\\programdata\\anaconda3\\lib\\site-packages (from accelerate) (5.9.0)\n",
      "Requirement already satisfied: pycparser in c:\\programdata\\anaconda3\\lib\\site-packages (from cffi>=1.0->soundfile) (2.21)\n",
      "Requirement already satisfied: llvmlite<0.41,>=0.40.0dev0 in c:\\programdata\\anaconda3\\lib\\site-packages (from numba>=0.51.0->librosa) (0.40.0)\n",
      "Requirement already satisfied: platformdirs>=2.5.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from pooch>=1.5->mne) (3.10.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->transformers) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->transformers) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->transformers) (2023.7.22)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from importlib-metadata->diffusers) (3.11.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from jinja2->torch) (2.1.1)\n"
     ]
    }
   ],
   "source": [
    "# 필요한 라이브러리 설치\n",
    "!pip install torch torchaudio numpy matplotlib scipy scikit-learn mne librosa soundfile tqdm transformers diffusers accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "사용 장치: cuda\n"
     ]
    }
   ],
   "source": [
    "# 기본 라이브러리 임포트\n",
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "from IPython.display import Audio, display\n",
    "\n",
    "# 랜덤 시드 설정\n",
    "RANDOM_SEED = 42\n",
    "np.random.seed(RANDOM_SEED)\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(RANDOM_SEED)\n",
    "\n",
    "# GPU 사용 가능 여부 확인\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"사용 장치: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "프로젝트 디렉토리: C:\\Users\\user\\Desktop\\jm\\eeg2speech_experiment\\eeg2speech\n",
      "코드 디렉토리: C:\\Users\\user\\Desktop\\jm\\eeg2speech_experiment\\eeg2speech\\code\n",
      "데이터 디렉토리: C:\\Users\\user\\Desktop\\jm\\eeg2speech_experiment\\eeg2speech\\data\n",
      "결과 디렉토리: C:\\Users\\user\\Desktop\\jm\\eeg2speech_experiment\\eeg2speech\\results\n",
      "모델 디렉토리: C:\\Users\\user\\Desktop\\jm\\eeg2speech_experiment\\eeg2speech\\models\n",
      "사전 학습된 모델 캐시 디렉토리: C:\\Users\\user\\Desktop\\jm\\eeg2speech_experiment\\eeg2speech\\pretrained_models\n"
     ]
    }
   ],
   "source": [
    "# 프로젝트 디렉토리 설정\n",
    "PROJECT_DIR = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "CODE_DIR = os.path.join(PROJECT_DIR, 'code')\n",
    "DATA_DIR = os.path.join(PROJECT_DIR, 'data')\n",
    "RESULTS_DIR = os.path.join(PROJECT_DIR, 'results')\n",
    "MODELS_DIR = os.path.join(PROJECT_DIR, 'models')\n",
    "PRETRAINED_CACHE_DIR = os.path.join(PROJECT_DIR, 'pretrained_models')\n",
    "\n",
    "# 디렉토리 생성\n",
    "os.makedirs(DATA_DIR, exist_ok=True)\n",
    "os.makedirs(RESULTS_DIR, exist_ok=True)\n",
    "os.makedirs(MODELS_DIR, exist_ok=True)\n",
    "os.makedirs(PRETRAINED_CACHE_DIR, exist_ok=True)\n",
    "\n",
    "# 코드 디렉토리를 시스템 경로에 추가\n",
    "if CODE_DIR not in sys.path:\n",
    "    sys.path.append(CODE_DIR)\n",
    "\n",
    "print(f\"프로젝트 디렉토리: {PROJECT_DIR}\")\n",
    "print(f\"코드 디렉토리: {CODE_DIR}\")\n",
    "print(f\"데이터 디렉토리: {DATA_DIR}\")\n",
    "print(f\"결과 디렉토리: {RESULTS_DIR}\")\n",
    "print(f\"모델 디렉토리: {MODELS_DIR}\")\n",
    "print(f\"사전 학습된 모델 캐시 디렉토리: {PRETRAINED_CACHE_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 데이터 다운로드 및 준비\n",
    "\n",
    "Inner Speech 데이터셋과 LibriSpeech 데이터셋을 다운로드하고 준비합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inner Speech 데이터셋 다운로드 중...\n",
      "이미 다운로드된 저장소가 있습니다: C:\\Users\\user\\Desktop\\jm\\eeg2speech_experiment\\eeg2speech\\data\\inner_speech\\Inner_Speech_Dataset\n",
      "저장소를 최신 버전으로 업데이트했습니다.\n",
      "OpenNeuro 데이터셋은 용량이 크므로 자동으로 다운로드하지 않습니다.\n",
      "필요한 경우 다음 URL에서 수동으로 다운로드하세요: https://openneuro.org/datasets/ds003626/versions/1.0.2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\user\\\\Desktop\\\\jm\\\\eeg2speech_experiment\\\\eeg2speech\\\\data\\\\inner_speech\\\\Inner_Speech_Dataset'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 데이터 다운로드 모듈 임포트\n",
    "from data_download import InnerSpeechDownloader, LibriSpeechDownloader\n",
    "\n",
    "# Inner Speech 데이터셋 다운로드\n",
    "inner_speech_dir = os.path.join(DATA_DIR, 'inner_speech')\n",
    "inner_speech_downloader = InnerSpeechDownloader(inner_speech_dir)\n",
    "inner_speech_downloader.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LibriSpeech dev-clean 데이터셋 다운로드 중...\n",
      "dev-clean 다운로드 중...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "330007KB [00:35, 9228.32KB/s]                                                                        \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dev-clean 압축 해제 중...\n",
      "LibriSpeech dev-clean 데이터셋 준비 완료: C:\\Users\\user\\Desktop\\jm\\eeg2speech_experiment\\eeg2speech\\data\\librispeech\\dev-clean\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\user\\\\Desktop\\\\jm\\\\eeg2speech_experiment\\\\eeg2speech\\\\data\\\\librispeech\\\\dev-clean'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# LibriSpeech 데이터셋 다운로드 (일부만 다운로드)\n",
    "librispeech_dir = os.path.join(DATA_DIR, 'librispeech')\n",
    "librispeech_downloader = LibriSpeechDownloader(librispeech_dir)\n",
    "librispeech_downloader.download(subset='dev-clean')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 데이터 탐색 및 시각화\n",
    "\n",
    "다운로드한 데이터셋을 탐색하고 시각화합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'EEGProcessor' object has no attribute 'load_data'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 8\u001b[0m\n\u001b[0;32m      5\u001b[0m eeg_processor \u001b[38;5;241m=\u001b[39m EEGProcessor(data_dir\u001b[38;5;241m=\u001b[39minner_speech_dir)\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# EEG 데이터 로드\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m eeg_data \u001b[38;5;241m=\u001b[39m eeg_processor\u001b[38;5;241m.\u001b[39mload_data()\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# EEG 데이터 정보 출력\u001b[39;00m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEEG 데이터 형태: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00meeg_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124meeg\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'EEGProcessor' object has no attribute 'load_data'"
     ]
    }
   ],
   "source": [
    "# 데이터 전처리 모듈 임포트\n",
    "from data_preprocessing import EEGProcessor, AudioProcessor, EEGAudioDataset\n",
    "\n",
    "# EEG 프로세서 생성\n",
    "eeg_processor = EEGProcessor(data_dir=inner_speech_dir)\n",
    "\n",
    "# EEG 데이터 로드\n",
    "eeg_data = eeg_processor.load_data()\n",
    "\n",
    "# EEG 데이터 정보 출력\n",
    "print(f\"EEG 데이터 형태: {eeg_data['eeg'].shape}\")\n",
    "print(f\"EEG 채널 수: {eeg_data['channels'].shape[0]}\")\n",
    "print(f\"EEG 샘플 수: {len(eeg_data['events'])}\")\n",
    "print(f\"EEG 이벤트 유형: {set(eeg_data['event_types'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'eeg_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# EEG 데이터 시각화\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m eeg_processor\u001b[38;5;241m.\u001b[39mvisualize_eeg(eeg_data, sample_idx\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, n_channels\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, title\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEEG 데이터 샘플\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'eeg_data' is not defined"
     ]
    }
   ],
   "source": [
    "# EEG 데이터 시각화\n",
    "eeg_processor.visualize_eeg(eeg_data, sample_idx=0, n_channels=10, title=\"EEG 데이터 샘플\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Roaming\\Python\\Python311\\site-packages\\huggingface_hub\\file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a6ef79d6d4145b99ccfabcf50e1c750",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "preprocessor_config.json:   0%|          | 0.00/537 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Roaming\\Python\\Python311\\site-packages\\huggingface_hub\\file_download.py:142: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\user\\.cache\\huggingface\\hub\\models--laion--clap-htsat-fused. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c63fd4936de41d9ae38b89f6f9311e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/384 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd4ec08f927c426498c1dc122c0e0912",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/798k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3fb6708a3ffb41c18355d0fdb574e453",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a95a9e90a80444a7addb25053981e3b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/2.11M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f7d9222f967546169501380419d1a155",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/280 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "경고: C:\\Users\\user\\Desktop\\jm\\eeg2speech_experiment\\eeg2speech\\data\\librispeech\\librispeech\\dev-clean 디렉토리를 찾을 수 없습니다.\n",
      "오디오 파일 수: 0\n",
      "오디오 파일 예시: []\n"
     ]
    }
   ],
   "source": [
    "# 오디오 프로세서 생성\n",
    "audio_processor = AudioProcessor(data_dir=librispeech_dir)\n",
    "\n",
    "# 오디오 파일 로드\n",
    "audio_files = audio_processor.get_audio_files()\n",
    "\n",
    "# 오디오 파일 정보 출력\n",
    "print(f\"오디오 파일 수: {len(audio_files)}\")\n",
    "print(f\"오디오 파일 예시: {audio_files[:5]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'AudioProcessor' object has no attribute 'visualize_audio'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# 오디오 데이터 시각화\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m audio_processor\u001b[38;5;241m.\u001b[39mvisualize_audio(audio_files[\u001b[38;5;241m0\u001b[39m], title\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m오디오 데이터 샘플\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# 오디오 재생\u001b[39;00m\n\u001b[0;32m      5\u001b[0m waveform, sample_rate \u001b[38;5;241m=\u001b[39m audio_processor\u001b[38;5;241m.\u001b[39mload_audio(audio_files[\u001b[38;5;241m0\u001b[39m])\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'AudioProcessor' object has no attribute 'visualize_audio'"
     ]
    }
   ],
   "source": [
    "# 오디오 데이터 시각화\n",
    "audio_processor.visualize_audio(audio_files[0], title=\"오디오 데이터 샘플\")\n",
    "\n",
    "# 오디오 재생\n",
    "waveform, sample_rate = audio_processor.load_audio(audio_files[0])\n",
    "display(Audio(waveform, rate=sample_rate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'eeg_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# 데이터셋 생성\u001b[39;00m\n\u001b[0;32m      2\u001b[0m dataset \u001b[38;5;241m=\u001b[39m EEGAudioDataset(\n\u001b[1;32m----> 3\u001b[0m     eeg_data\u001b[38;5;241m=\u001b[39meeg_data,\n\u001b[0;32m      4\u001b[0m     audio_files\u001b[38;5;241m=\u001b[39maudio_files,\n\u001b[0;32m      5\u001b[0m     eeg_processor\u001b[38;5;241m=\u001b[39meeg_processor,\n\u001b[0;32m      6\u001b[0m     audio_processor\u001b[38;5;241m=\u001b[39maudio_processor\n\u001b[0;32m      7\u001b[0m )\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m데이터셋 크기: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(dataset)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'eeg_data' is not defined"
     ]
    }
   ],
   "source": [
    "# 데이터셋 생성\n",
    "dataset = EEGAudioDataset(\n",
    "    eeg_data=eeg_data,\n",
    "    audio_files=audio_files,\n",
    "    eeg_processor=eeg_processor,\n",
    "    audio_processor=audio_processor\n",
    ")\n",
    "\n",
    "print(f\"데이터셋 크기: {len(dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# 데이터셋 샘플 확인\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m sample \u001b[38;5;241m=\u001b[39m dataset[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEEG 데이터 형태: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msample[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124meeg\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m오디오 데이터 형태: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msample[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maudio\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'dataset' is not defined"
     ]
    }
   ],
   "source": [
    "# 데이터셋 샘플 확인\n",
    "sample = dataset[0]\n",
    "print(f\"EEG 데이터 형태: {sample['eeg'].shape}\")\n",
    "print(f\"오디오 데이터 형태: {sample['audio'].shape}\")\n",
    "print(f\"멜 스펙트로그램 형태: {sample['mel_spectrogram'].shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 모델 구성 요소 초기화\n",
    "\n",
    "EEG 인코더, 잠재 벡터 매핑 모델, 사전 학습된 모델을 초기화합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\anaconda3\\Lib\\site-packages\\transformers\\utils\\generic.py:260: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n",
      "C:\\ProgramData\\anaconda3\\Lib\\site-packages\\transformers\\utils\\generic.py:260: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Failed to import diffusers.pipelines.audioldm2.pipeline_audioldm2 because of the following error (look up to see its traceback):\ncannot import name 'VitsModel' from 'transformers' (C:\\ProgramData\\anaconda3\\Lib\\site-packages\\transformers\\__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\diffusers\\utils\\import_utils.py:920\u001b[0m, in \u001b[0;36m_LazyModule._get_module\u001b[1;34m(self, module_name)\u001b[0m\n\u001b[0;32m    919\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 920\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m importlib\u001b[38;5;241m.\u001b[39mimport_module(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m module_name, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)\n\u001b[0;32m    921\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\importlib\\__init__.py:126\u001b[0m, in \u001b[0;36mimport_module\u001b[1;34m(name, package)\u001b[0m\n\u001b[0;32m    125\u001b[0m         level \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m--> 126\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _bootstrap\u001b[38;5;241m.\u001b[39m_gcd_import(name[level:], package, level)\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1204\u001b[0m, in \u001b[0;36m_gcd_import\u001b[1;34m(name, package, level)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1176\u001b[0m, in \u001b[0;36m_find_and_load\u001b[1;34m(name, import_)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1147\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[1;34m(name, import_)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:690\u001b[0m, in \u001b[0;36m_load_unlocked\u001b[1;34m(spec)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:940\u001b[0m, in \u001b[0;36mexec_module\u001b[1;34m(self, module)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:241\u001b[0m, in \u001b[0;36m_call_with_frames_removed\u001b[1;34m(f, *args, **kwds)\u001b[0m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\diffusers\\pipelines\\audioldm2\\pipeline_audioldm2.py:20\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m---> 20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     21\u001b[0m     ClapFeatureExtractor,\n\u001b[0;32m     22\u001b[0m     ClapModel,\n\u001b[0;32m     23\u001b[0m     GPT2Model,\n\u001b[0;32m     24\u001b[0m     RobertaTokenizer,\n\u001b[0;32m     25\u001b[0m     RobertaTokenizerFast,\n\u001b[0;32m     26\u001b[0m     SpeechT5HifiGan,\n\u001b[0;32m     27\u001b[0m     T5EncoderModel,\n\u001b[0;32m     28\u001b[0m     T5Tokenizer,\n\u001b[0;32m     29\u001b[0m     T5TokenizerFast,\n\u001b[0;32m     30\u001b[0m     VitsModel,\n\u001b[0;32m     31\u001b[0m     VitsTokenizer,\n\u001b[0;32m     32\u001b[0m )\n\u001b[0;32m     34\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AutoencoderKL\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'VitsModel' from 'transformers' (C:\\ProgramData\\anaconda3\\Lib\\site-packages\\transformers\\__init__.py)",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01meeg_encoder\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m create_eeg_encoder\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlatent_mapping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m create_latent_mapping_model\n\u001b[1;32m----> 4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpretrained_models\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m create_pretrained_model_manager, PretrainedModelIntegrator\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01meeg2speech_trainer\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m create_eeg2speech_trainer\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01meeg2speech_pipeline\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m EEG2SpeechPipeline\n",
      "File \u001b[1;32m~\\Desktop\\jm\\eeg2speech_experiment\\eeg2speech\\code\\pretrained_models.py:17\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     10\u001b[0m     AutoModel, \n\u001b[0;32m     11\u001b[0m     AutoProcessor, \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     15\u001b[0m     WhisperForConditionalGeneration\n\u001b[0;32m     16\u001b[0m )\n\u001b[1;32m---> 17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdiffusers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AudioLDM2Pipeline, AudioLDMPipeline\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorchaudio\u001b[39;00m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtqdm\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tqdm\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1229\u001b[0m, in \u001b[0;36m_handle_fromlist\u001b[1;34m(module, fromlist, import_, recursive)\u001b[0m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\diffusers\\utils\\import_utils.py:911\u001b[0m, in \u001b[0;36m_LazyModule.__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m    909\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_class_to_module\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[0;32m    910\u001b[0m     module \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_module(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_class_to_module[name])\n\u001b[1;32m--> 911\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(module, name)\n\u001b[0;32m    912\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    913\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodule \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m has no attribute \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\diffusers\\utils\\import_utils.py:911\u001b[0m, in \u001b[0;36m_LazyModule.__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m    909\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_class_to_module\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[0;32m    910\u001b[0m     module \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_module(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_class_to_module[name])\n\u001b[1;32m--> 911\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(module, name)\n\u001b[0;32m    912\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    913\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodule \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m has no attribute \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\diffusers\\utils\\import_utils.py:910\u001b[0m, in \u001b[0;36m_LazyModule.__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m    908\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_module(name)\n\u001b[0;32m    909\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_class_to_module\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m--> 910\u001b[0m     module \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_module(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_class_to_module[name])\n\u001b[0;32m    911\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(module, name)\n\u001b[0;32m    912\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\diffusers\\utils\\import_utils.py:922\u001b[0m, in \u001b[0;36m_LazyModule._get_module\u001b[1;34m(self, module_name)\u001b[0m\n\u001b[0;32m    920\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m importlib\u001b[38;5;241m.\u001b[39mimport_module(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m module_name, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)\n\u001b[0;32m    921\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m--> 922\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m    923\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to import \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodule_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m because of the following error (look up to see its\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    924\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m traceback):\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    925\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Failed to import diffusers.pipelines.audioldm2.pipeline_audioldm2 because of the following error (look up to see its traceback):\ncannot import name 'VitsModel' from 'transformers' (C:\\ProgramData\\anaconda3\\Lib\\site-packages\\transformers\\__init__.py)"
     ]
    }
   ],
   "source": [
    "# 모델 모듈 임포트\n",
    "from eeg_encoder import create_eeg_encoder\n",
    "from latent_mapping import create_latent_mapping_model\n",
    "from pretrained_models import create_pretrained_model_manager, PretrainedModelIntegrator\n",
    "from eeg2speech_trainer import create_eeg2speech_trainer\n",
    "from eeg2speech_pipeline import EEG2SpeechPipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "설정 저장 완료: C:\\Users\\user\\Desktop\\jm\\eeg2speech_experiment\\eeg2speech\\config.json\n"
     ]
    }
   ],
   "source": [
    "# 설정 정의\n",
    "config = {\n",
    "    'eeg': {\n",
    "        'n_channels': 128,  # EEG 채널 수\n",
    "        'n_times': 2000,    # EEG 시간 포인트 수\n",
    "        'vae_latent_dim': 512,  # VAE 잠재 공간 차원 (AudioLDM2와 호환)\n",
    "        'clap_latent_dim': 512,  # CLAP 잠재 공간 차원 (CLAP와 호환)\n",
    "        'hidden_dims': [64, 128, 256, 512]  # 은닉층 차원\n",
    "    },\n",
    "    'mapping': {\n",
    "        'mapping_type': 'mlp',  # 매핑 모델 유형 ('mlp', 'resnet', 'adversarial')\n",
    "        'hidden_dims': [1024, 1024, 512],  # 은닉층 차원\n",
    "        'dropout': 0.2  # 드롭아웃 비율\n",
    "    },\n",
    "    'training': {\n",
    "        'batch_size': 32,  # 배치 크기\n",
    "        'val_split': 0.2,  # 검증 세트 비율\n",
    "        'test_split': 0.1,  # 테스트 세트 비율\n",
    "        'random_seed': 42,  # 랜덤 시드\n",
    "        'encoder_epochs': 100,  # EEG 인코더 에폭 수\n",
    "        'mapping_epochs': 100,  # 잠재 벡터 매핑 모델 에폭 수\n",
    "        'encoder_lr': 1e-4,  # EEG 인코더 학습률\n",
    "        'mapping_lr': 1e-4,  # 잠재 벡터 매핑 모델 학습률\n",
    "        'patience': 10  # 조기 종료 인내심\n",
    "    },\n",
    "    'generation': {\n",
    "        'prompt': 'speech, clear voice',  # 텍스트 프롬프트\n",
    "        'num_inference_steps': 50,  # 추론 단계 수\n",
    "        'audio_length_in_s': 5.0  # 생성할 오디오 길이 (초)\n",
    "    },\n",
    "    'paths': {\n",
    "        'data_dir': DATA_DIR,  # 데이터 디렉토리\n",
    "        'output_dir': RESULTS_DIR,  # 결과 디렉토리\n",
    "        'model_dir': MODELS_DIR,  # 모델 디렉토리\n",
    "        'pretrained_cache_dir': PRETRAINED_CACHE_DIR  # 사전 학습된 모델 캐시 디렉토리\n",
    "    }\n",
    "}\n",
    "\n",
    "# 설정 저장\n",
    "config_path = os.path.join(PROJECT_DIR, 'config.json')\n",
    "with open(config_path, 'w') as f:\n",
    "    json.dump(config, f, indent=4)\n",
    "\n",
    "print(f\"설정 저장 완료: {config_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'EEG2SpeechPipeline' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# EEG2Speech 파이프라인 생성\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m pipeline \u001b[38;5;241m=\u001b[39m EEG2SpeechPipeline(config_path\u001b[38;5;241m=\u001b[39mconfig_path, device\u001b[38;5;241m=\u001b[39mdevice)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEEG2Speech 파이프라인 생성 완료\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'EEG2SpeechPipeline' is not defined"
     ]
    }
   ],
   "source": [
    "# EEG2Speech 파이프라인 생성\n",
    "pipeline = EEG2SpeechPipeline(config_path=config_path, device=device)\n",
    "\n",
    "print(\"EEG2Speech 파이프라인 생성 완료\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 모델 학습\n",
    "\n",
    "EEG 인코더와 잠재 벡터 매핑 모델을 학습합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pipeline' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# 데이터셋 분할\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m train_dataset, val_dataset, test_dataset \u001b[38;5;241m=\u001b[39m pipeline\u001b[38;5;241m.\u001b[39msplit_dataset(dataset)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m학습 데이터셋 크기: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(train_dataset)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m검증 데이터셋 크기: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(val_dataset)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'pipeline' is not defined"
     ]
    }
   ],
   "source": [
    "# 데이터셋 분할\n",
    "train_dataset, val_dataset, test_dataset = pipeline.split_dataset(dataset)\n",
    "\n",
    "print(f\"학습 데이터셋 크기: {len(train_dataset)}\")\n",
    "print(f\"검증 데이터셋 크기: {len(val_dataset)}\")\n",
    "print(f\"테스트 데이터셋 크기: {len(test_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pipeline' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# EEG 인코더 학습\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m encoder_history \u001b[38;5;241m=\u001b[39m pipeline\u001b[38;5;241m.\u001b[39mtrain_eeg_encoder(train_dataset, val_dataset)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# 학습 곡선 시각화\u001b[39;00m\n\u001b[0;32m      5\u001b[0m pipeline\u001b[38;5;241m.\u001b[39mvisualize_training_history(encoder_history, title\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEEG 인코더 학습 곡선\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'pipeline' is not defined"
     ]
    }
   ],
   "source": [
    "# EEG 인코더 학습\n",
    "encoder_history = pipeline.train_eeg_encoder(train_dataset, val_dataset)\n",
    "\n",
    "# 학습 곡선 시각화\n",
    "pipeline.visualize_training_history(encoder_history, title=\"EEG 인코더 학습 곡선\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pipeline' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[21], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# 잠재 벡터 매핑 모델 학습\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m mapping_history \u001b[38;5;241m=\u001b[39m pipeline\u001b[38;5;241m.\u001b[39mtrain_latent_mapping(train_dataset, val_dataset)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# 학습 곡선 시각화\u001b[39;00m\n\u001b[0;32m      5\u001b[0m pipeline\u001b[38;5;241m.\u001b[39mvisualize_training_history(mapping_history, title\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m잠재 벡터 매핑 모델 학습 곡선\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'pipeline' is not defined"
     ]
    }
   ],
   "source": [
    "# 잠재 벡터 매핑 모델 학습\n",
    "mapping_history = pipeline.train_latent_mapping(train_dataset, val_dataset)\n",
    "\n",
    "# 학습 곡선 시각화\n",
    "pipeline.visualize_training_history(mapping_history, title=\"잠재 벡터 매핑 모델 학습 곡선\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. 모델 평가\n",
    "\n",
    "학습된 모델을 테스트 데이터셋에서 평가합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pipeline' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[22], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# 모델 평가\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m evaluation_results \u001b[38;5;241m=\u001b[39m pipeline\u001b[38;5;241m.\u001b[39mevaluate(test_dataset)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# 평가 결과 출력\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m평가 결과:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'pipeline' is not defined"
     ]
    }
   ],
   "source": [
    "# 모델 평가\n",
    "evaluation_results = pipeline.evaluate(test_dataset)\n",
    "\n",
    "# 평가 결과 출력\n",
    "print(\"평가 결과:\")\n",
    "for metric, value in evaluation_results.items():\n",
    "    print(f\"{metric}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pipeline' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[23], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# 잠재 공간 시각화\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m pipeline\u001b[38;5;241m.\u001b[39mvisualize_latent_space(test_dataset, title\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m잠재 공간 시각화\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'pipeline' is not defined"
     ]
    }
   ],
   "source": [
    "# 잠재 공간 시각화\n",
    "pipeline.visualize_latent_space(test_dataset, title=\"잠재 공간 시각화\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. 결과 시각화 및 분석\n",
    "\n",
    "생성된 오디오 샘플을 시각화하고 분석합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'test_dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[24], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# 테스트 샘플에서 오디오 생성\u001b[39;00m\n\u001b[0;32m      2\u001b[0m sample_idx \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m----> 3\u001b[0m sample \u001b[38;5;241m=\u001b[39m test_dataset[sample_idx]\n\u001b[0;32m      4\u001b[0m eeg_data \u001b[38;5;241m=\u001b[39m sample[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124meeg\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# 오디오 생성\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'test_dataset' is not defined"
     ]
    }
   ],
   "source": [
    "# 테스트 샘플에서 오디오 생성\n",
    "sample_idx = 0\n",
    "sample = test_dataset[sample_idx]\n",
    "eeg_data = sample['eeg'].unsqueeze(0).to(device)\n",
    "\n",
    "# 오디오 생성\n",
    "generated_audio, sample_rate = pipeline.generate_audio_from_eeg(eeg_data)\n",
    "\n",
    "# 생성된 오디오 재생\n",
    "display(Audio(generated_audio.cpu().numpy(), rate=sample_rate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sample' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[25], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# 원본 오디오와 생성된 오디오 비교\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m original_audio \u001b[38;5;241m=\u001b[39m sample[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maudio\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[0;32m      3\u001b[0m generated_audio \u001b[38;5;241m=\u001b[39m generated_audio\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[0;32m      5\u001b[0m plt\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m12\u001b[39m, \u001b[38;5;241m6\u001b[39m))\n",
      "\u001b[1;31mNameError\u001b[0m: name 'sample' is not defined"
     ]
    }
   ],
   "source": [
    "# 원본 오디오와 생성된 오디오 비교\n",
    "original_audio = sample['audio'].cpu().numpy()\n",
    "generated_audio = generated_audio.cpu().numpy()\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.title(\"원본 오디오 파형\")\n",
    "plt.plot(original_audio)\n",
    "\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.title(\"생성된 오디오 파형\")\n",
    "plt.plot(generated_audio)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'test_dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[26], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# 여러 샘플에 대한 오디오 생성 및 저장\u001b[39;00m\n\u001b[0;32m      2\u001b[0m num_samples \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m5\u001b[39m\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mmin\u001b[39m(num_samples, \u001b[38;5;28mlen\u001b[39m(test_dataset))):\n\u001b[0;32m      4\u001b[0m     sample \u001b[38;5;241m=\u001b[39m test_dataset[i]\n\u001b[0;32m      5\u001b[0m     eeg_data \u001b[38;5;241m=\u001b[39m sample[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124meeg\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'test_dataset' is not defined"
     ]
    }
   ],
   "source": [
    "# 여러 샘플에 대한 오디오 생성 및 저장\n",
    "num_samples = 5\n",
    "for i in range(min(num_samples, len(test_dataset))):\n",
    "    sample = test_dataset[i]\n",
    "    eeg_data = sample['eeg'].unsqueeze(0).to(device)\n",
    "    \n",
    "    # 오디오 생성\n",
    "    generated_audio, sample_rate = pipeline.generate_audio_from_eeg(eeg_data)\n",
    "    \n",
    "    # 오디오 저장\n",
    "    output_path = os.path.join(RESULTS_DIR, f\"generated_audio_{i}.wav\")\n",
    "    pipeline.save_audio(generated_audio, sample_rate, output_path)\n",
    "    \n",
    "    print(f\"샘플 {i}의 생성된 오디오 저장 완료: {output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. 결론 및 향후 연구 방향\n",
    "\n",
    "프로젝트의 결론과 향후 연구 방향을 정리합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pipeline' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[27], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# 모델 저장\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m pipeline\u001b[38;5;241m.\u001b[39msave_model(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(MODELS_DIR, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124meeg2speech_model.pt\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m모델 저장 완료\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'pipeline' is not defined"
     ]
    }
   ],
   "source": [
    "# 모델 저장\n",
    "pipeline.save_model(os.path.join(MODELS_DIR, \"eeg2speech_model.pt\"))\n",
    "print(\"모델 저장 완료\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 결론\n",
    "\n",
    "이 프로젝트에서는 EEG 데이터에서 음성을 생성하는 파이프라인을 구현했습니다. 주요 구성 요소는 다음과 같습니다:\n",
    "\n",
    "1. **이중 경로 EEG 인코더**: EEG 데이터를 잠재 표현으로 변환\n",
    "2. **잠재 벡터 매핑 모델**: EEG 잠재 벡터를 오디오 잠재 벡터로 매핑\n",
    "3. **사전 학습된 모델 활용**: AudioLDM2, CLAP, Whisper 등의 사전 학습된 모델 활용\n",
    "\n",
    "### 향후 연구 방향\n",
    "\n",
    "1. **더 큰 데이터셋 활용**: 더 많은 EEG 데이터와 오디오 데이터를 활용하여 모델 성능 향상\n",
    "2. **실시간 처리**: 실시간 EEG 신호에서 음성을 생성하는 시스템 개발\n",
    "3. **개인화**: 개인별 특성을 반영한 맞춤형 모델 개발\n",
    "4. **다양한 언어 지원**: 다양한 언어에 대한 지원 확장\n",
    "5. **하이브리드 접근 방식**: EEG와 다른 생체 신호를 결합한 하이브리드 접근 방식 탐구"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
